{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ce678df",
   "metadata": {},
   "source": [
    "# Data Cleaning & Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cfd9b4c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "  \n",
    "<b>Notebook objectives:</b>\n",
    "    \n",
    "* Define a data cleaning function and transform raw form data set\n",
    "    \n",
    "    \n",
    "* Define a data preprocessing pipeline and transform cleaned data set\n",
    "    \n",
    "    \n",
    "* Export X and y preprocessed data in pickle form ready for ML modeling    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d865d66",
   "metadata": {},
   "source": [
    "# 1. Notebook set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f17f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try downgrading pandas version if pcikle throws an error while loading\n",
    "# !pip install pandas==1.4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5140dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Import packages\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "#from IPython.display import HTML, Image #display formatted texts\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Plotting packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn') # pretty graphs\n",
    "import matplotlib.ticker as mticker\n",
    "from matplotlib.ticker import FormatStrFormatter, StrMethodFormatter, FuncFormatter\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# Files to pickle\n",
    "import pickle\n",
    "import bz2\n",
    "import _pickle as cPickle\n",
    "\n",
    "# sampling\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler,OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "# Models\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, average_precision_score\n",
    "from sklearn.inspection import partial_dependence\n",
    "\n",
    "# Load bars\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Path set up\n",
    "path = \"/project/data/\"\n",
    "path_w1 = \"/project/data/w1/\"\n",
    "path_w2 = \"/project/data/w2/\"\n",
    "path_w3 = \"/project/data/w3/\"\n",
    "path_w4 = \"/project/data/w4/\"\n",
    "path_w5 = \"/project/data/w5/\"\n",
    "path_feature = \"/project/data/feature_importance/\"\n",
    "json_path = \"/project/notebooks/map/KEYFILE.json\"\n",
    "\n",
    "# Fixed values\n",
    "seed = 2323\n",
    "colors = {'c1':['blue', 'red'], 'c2': ['red', 'blue', 'grey', 'purple']}\n",
    "bar_width = 0.3\n",
    "bin_num = 25\n",
    "size = {'small_tick': 9, 'tick': 10 , 'label': 14, 'sub_title': 16, 'main_title': 20}\n",
    "fig_size = {'large': (30,25), 'small': (10, 5)}\n",
    "\n",
    "# pandas display set up\n",
    "pd.options.display.max_columns = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a9095",
   "metadata": {},
   "source": [
    "### 1.1 Load samples data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d95f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading pickle files that will be clean and preprocess for ML modeling\n",
    "\n",
    "# load pickled data for w2\n",
    "pickled_data = bz2.BZ2File(path_w2 + 'strata_target_w2_1pct', 'rb')\n",
    "sample_df = cPickle.load(pickled_data)\n",
    "sample_df.drop(list(sample_df.filter(regex = 'ID')), axis = 1, inplace = True) # removes ID columns that do not hold explanatorry value\n",
    "pickled_data.close()\n",
    "# n_df = pd.concat([w1_df,w2_df], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06719aae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample data set size:\n",
      "397979 rows\n",
      "42 columns\n"
     ]
    }
   ],
   "source": [
    "# make a new reference to the data set\n",
    "data = sample_df.reset_index()\n",
    "print(F'Sample data set size:\\n\\\n",
    "{data.shape[0]} rows\\n\\\n",
    "{data.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56be13bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Loading pickle file for droppped prep df\n",
    "\n",
    "# load pickled data\n",
    "pickled_data = bz2.BZ2File(path_feature + 'df_prep_dropped', 'rb')\n",
    "df_prep_dropped = cPickle.load(pickled_data)\n",
    "#df_prep_dropped.drop(list(sample_df.filter(regex = 'ID')), axis = 1, inplace = True) # removes ID columns that do not hold explanatorry value\n",
    "pickled_data.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ae78fd",
   "metadata": {},
   "source": [
    "# 2. Data cleaning and pre-processing for ML modeling\n",
    "\n",
    "Defining a data cleaning function that later will be used as input into a transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3777d3f1",
   "metadata": {},
   "source": [
    "### 2.1 Data Cleaning function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5ed61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data_step(df, ordinal_apply = True):\n",
    "    \n",
    "    # 1. mapping features renamed\n",
    "    \n",
    "    # mapper to rename columns\n",
    "    dict_features = {'Advertiser Currency': 'ad_currency', 'Date': 'date', 'Time of Day': 'tod', 'Advertiser': 'advertiser_name', 'Campaign': 'campaign_name', \n",
    "                    'Insertion Order': 'insertion_order', 'Line Item': 'line_item', 'Line Item Type': 'line_item_type', 'Platform': 'platform',\n",
    "                    'Device Type': 'device_type', 'Device Make': 'device_make', 'Device Model': 'device_model', 'Operating System': 'os', 'Browser': 'browser',\n",
    "                    'ISP or Carrier': 'isp_carrier', 'Environment': 'environment', 'Creative Type': 'creative_type', 'Creative': 'creative_name', 'Creative Size': 'creative_size',\n",
    "                    'App/URL': 'app_url', 'Channel Type': 'channel_type', 'Channel': 'channel_name', 'Exchange': 'exchange', 'Inventory Source': 'inventory_source',\n",
    "                    'Ad Position': 'ad_position', 'Ad Type': 'ad_type', 'Inventory Source Type': 'inventory_source_type', 'Position in Content': 'position_in_content',\n",
    "                    'Public Inventory': 'public_inventory', 'Country': 'country', 'City': 'city', 'Impressions': 'impressions', 'Billable Impressions': 'billable_impressions',\n",
    "                    'Active View: Viewable Impressions': 'viewable_impressions', 'Clicks': 'clicks', 'Total Conversions': 'total_conversions', \n",
    "                    'Post-Click Conversions': 'post_click_conversions', 'Post-View Conversions': 'post_view_conversions', \n",
    "                    'Revenue (Adv Currency)': 'media_cost', 'Media Cost (Advertiser Currency)': 'total_media_cost'}\n",
    "    \n",
    "    # rename columns to ease future referencing\n",
    "    df.rename(columns = dict_features, inplace = True)\n",
    "    \n",
    "    # 2. Truncated instances are dropped which represent ~0.00008% fo total data set size\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # 3. Droppping unique value categorical features\n",
    "    unique_v_df = pd.DataFrame(df.nunique()).rename(columns={0: \"unique_values_count\"})\n",
    "    drop_unique_list = unique_v_df[unique_v_df['unique_values_count'] == 1].index.values.tolist()\n",
    "    df.drop(drop_unique_list, axis=1, inplace = True)\n",
    "    \n",
    "    # 4. changing data timestamp format\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "    # 5. setting day/month/year format\n",
    "    df['date'] = df['date'].dt.strftime('%d/%m/%Y')\n",
    "    # return as date timestampe format\n",
    "    df['date'] = pd.to_datetime(df['date'], format = \"%d/%m/%Y\")\n",
    "    \n",
    "    # 6. transforming floats to integers\n",
    "    exclude_num_cols = ['media_cost','total_media_cost','index']\n",
    "    float_cols = [var for var in df.select_dtypes(include=['int64', 'float64']).columns if var not in exclude_num_cols]\n",
    "    \n",
    "    # 7. transfor floats to int\n",
    "    try:\n",
    "        for col in float_cols:\n",
    "            df[col] = df[col].astype('int64')\n",
    "    except ValueError as excp:\n",
    "        print('Make sure to pass columns with float dtype only') # friendly error message\n",
    "        print(excp) # technical message as output\n",
    "        print(type(excp)) # type error as output\n",
    "    else:\n",
    "        pass#print(F'{len(float_cols)} features tranformed to int64 dtype')\n",
    "        \n",
    "        \n",
    "    # 8. Anonymizing data\n",
    "    \n",
    "    # insertion order\n",
    "    insertion_orders = {n:\"insertion_order{}\".format(i+1) for i, n in enumerate(df['insertion_order'].unique())}\n",
    "    df[\"insertion_order\"] = df['insertion_order'].map(insertion_orders)\n",
    "    \n",
    "    # anonymizing line items\n",
    "    line_items = {n:\"line_item{}\".format(i+1) for i, n in enumerate(df['line_item'].unique())}\n",
    "    df[\"line_item\"] = df['line_item'].map(line_items)\n",
    "    \n",
    "    # anonymizing creatives\n",
    "    creatives = {n:\"creative_name{}\".format(i+1) for i, n in enumerate(df['creative_name'].unique())}\n",
    "    df[\"creative_name\"] = df['creative_name'].map(creatives)\n",
    "    \n",
    "    \n",
    "    # 9. converting to string data types\n",
    "    df['day_of_week'] = df['date'].dt.day_name()\n",
    "    df['date'] = df['date'].dt.strftime('%d/%m/%Y')\n",
    "    \n",
    "    mapped_times = {n:\"tod_{}\".format(i+1) for i, n in enumerate(df['tod'].unique())}\n",
    "    df[\"tod\"] = df['tod'].map(mapped_times)\n",
    "\n",
    "    \n",
    "    # 10. Get the target feature\n",
    "    df['user_response'] = np.where(df['clicks'] == 0, 0, 1) # generates target feature\n",
    "    \n",
    "    # 11. log transform\n",
    "    log_transform_list = ['media_cost', 'total_media_cost']\n",
    "    \n",
    "    for col in log_transform_list:\n",
    "        df[col+'_log'] = np.log(df[col]+0.000001)\n",
    "        \n",
    "        df.drop([col], axis = 1, inplace = True)\n",
    "        \n",
    "    df.reset_index(drop = True, inplace =  True)\n",
    "        \n",
    "    # 12. Apply OrdinalEncoding if true\n",
    "    \n",
    "    if ordinal_apply:\n",
    "        \n",
    "        ### iter over variables\n",
    "        catg_cols_1 = ['device_model', 'app_url', 'city']\n",
    "        order_dict = {}\n",
    "\n",
    "        for idx, var in enumerate(catg_cols_1):\n",
    "\n",
    "            ### creating data form to plot\n",
    "            temp_df = df[[var]] # get feature selected as a df\n",
    "            temp_df = df.groupby(var, as_index = False).size().rename(columns={'size': 'count'})\n",
    "\n",
    "            ### computing distribution % frequency \n",
    "            temp_df['distribution'] = temp_df['count']/temp_df['count'].sum()\n",
    "            temp_df.sort_values(by = 'distribution', inplace = True, ascending = False)\n",
    "            order_dict[idx+1] = temp_df[var].values.tolist() # append ordered categorical values\n",
    "\n",
    "\n",
    "        # Categoric and numeric features\n",
    "        \n",
    "        ordinal_encoding = make_column_transformer(\n",
    "            (OrdinalEncoder(categories=[order_dict[1]]), ['device_model']),\n",
    "            (OrdinalEncoder(categories=[order_dict[2]]), ['app_url']),\n",
    "            (OrdinalEncoder(categories=[order_dict[3]]), ['city'])\n",
    "        )\n",
    "\n",
    "        encoding_df = pd.DataFrame(ordinal_encoding.fit_transform(df), columns=df[catg_cols_1].columns)\n",
    "\n",
    "        df.drop(columns = catg_cols_1, inplace = True)\n",
    "        df = pd.concat([df, encoding_df], axis = 1)\n",
    "    \n",
    "    #return cleaned df\n",
    "    print(f'cleaned data set with {df.shape[0]} rows and {df.shape[1]} columns.')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6f541b",
   "metadata": {},
   "source": [
    "### 2.3 Data cleaning using a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1cd9292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining custom cleaning step transformer\n",
    "datacleaning = FunctionTransformer(clean_data_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b488fd5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaned data set with 397979 rows and 31 columns.\n"
     ]
    }
   ],
   "source": [
    "# fitting cleaning step transformer\n",
    "df_cleaned = datacleaning.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ca6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run when prepping Pearson Regularized df\n",
    "# df_cleaned = df_prep_dropped"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9ee6b6",
   "metadata": {},
   "source": [
    "### 2.4 Defining X and y for preprocessing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "be4e2f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y matrices\n",
    "X = df_cleaned.drop(['user_response','clicks', 'index'],axis=1)\n",
    "y = df_cleaned['user_response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "611b2ad3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5931"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check point unique values in media cot log variable\n",
    "len(X['media_cost_log'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1643aaf5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check point matrix form\n",
    "X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c907561",
   "metadata": {},
   "source": [
    "### 2.5 Defining Pipeline steps for categorical and numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "344622d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Defining data pipeline\n",
    "\n",
    "exclude_list = ['index']\n",
    "\n",
    "# Categoric and numeric features                \n",
    "numeric_cols = [var for var in X.select_dtypes(include=['int64', 'float64']).columns if var not in exclude_list]\n",
    "catg_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# transform steps numeric\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "    ,('std_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# transform steps categoric\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "       ('imputer', SimpleImputer(strategy='constant'))\n",
    "      ,('onehot', OneHotEncoder(drop = 'first', sparse = False)) #swith true when a sparse matrix is required\n",
    "])\n",
    "\n",
    "# complete pipeline\n",
    "pipeline = ColumnTransformer([\n",
    "    ('numeric_pipeline', numeric_transformer, numeric_cols),\n",
    "    ('categoric_pipeline', categorical_transformer,catg_cols)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ee19a2",
   "metadata": {},
   "source": [
    "### 2.6 Applying data preprocessing pipeline to X\n",
    "\n",
    "**Note**: The output of X of the preprocessing has the form of a sparse matrix given the one hot encoding of the categorical variables with multiple unique values (e.g., the url_app feature has ~4K unique values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18293f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit data pipeline\n",
    "X = pipeline.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65660823",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### ONLY RUN WHEN USING A DENSE MATRIX FORM TO MAP BACK FEATURE NAMES #####\n",
    "\n",
    "n = pipeline.transformers_[0][2] # mapping numeric\n",
    "c = pipeline.transformers_[1][1]\\\n",
    "['onehot'].get_feature_names(catg_cols).tolist() # mapping categorical\n",
    "full = n + c # mapping complete columns list\n",
    "\n",
    "X = pd.DataFrame(X, columns = full) # getting df form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f65042db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0] == y.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e86cef",
   "metadata": {},
   "source": [
    "### 2.7 Define X and y data set split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4ec0c7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:16<00:00,  8.28s/it]\n"
     ]
    }
   ],
   "source": [
    "# transform train and test files to pickle form\n",
    "export_files = {1:(X, 'X_complete'), 2:(y,'y_complete')}\n",
    "\n",
    "for i in tqdm(export_files):\n",
    "    sfile = bz2.BZ2File(path_feature + export_files[i][1] +'_w2_1pct', 'w') #'_w1_1pct'\n",
    "    pickle.dump(export_files[i][0],sfile)\n",
    "    sfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02566829",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state = seed, stratify = y)\n",
    "print(F\"X_train matrix: {X_train.shape}\\ny_train vector:  {y_train.shape[0]}\\nX_test matrix: {X_test.shape}\\ny_test vector:  {y_test.shape[0]} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cadd12b3",
   "metadata": {},
   "source": [
    "# 3. Export preprocessed train and test ready for ML modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db863e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform train and test files to pickle form\n",
    "export_files = {1:(X_train, 'X_train'), 2:(y_train,'y_train'), 3: (X_test,'X_test'), 4: (y_test,'y_test')}\n",
    "\n",
    "for i in tqdm(export_files):\n",
    "    sfile = bz2.BZ2File(path_w2 + export_files[i][1] +'_dense_w2_1pct', 'w') #'_w1_1pct'\n",
    "    pickle.dump(export_files[i][0],sfile)\n",
    "    sfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc13dc4a",
   "metadata": {},
   "source": [
    "# 4. Train a simple classifier to test X_train and y_train\n",
    "\n",
    "**Note**: If the X sparse matrix has a very large form the server might run out of RAM, which would require a more powerful machine in place to train a simple classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4454ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickled_data = bz2.BZ2File(path + 'X_cleaned_w1', 'rb')\n",
    "w1_df = cPickle.load(pickled_data)\n",
    "w1_df.drop(list(w1_df.filter(regex = 'ID')), axis = 1, inplace = True) # removes ID columns that do not hold explanatorry value\n",
    "pickled_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544f3066",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_training = []\n",
    "\n",
    "for i in tqdm(range(1), desc = 'Progress cross-validation'):\n",
    "    # Training a naive classifier logit regression\n",
    "    # Using 5 fold cross-validation\n",
    "    first_training.append(cross_val_score(LogisticRegression(random_state = seed),\n",
    "                          X = X_train,\n",
    "                          y = y_train,\n",
    "                          cv = 5, # 5 fold cross validation\n",
    "                          #verbose = 5,\n",
    "                          scoring = 'roc_auc'))\n",
    "    time.sleep(0.5)\n",
    "    print('Logistic regression trained')\n",
    "    \n",
    "first_training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026cfe87",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5daad85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform train and test files to pickle form\n",
    "export_files = {1:(X, 'X'), 2:(y,'y')}\n",
    "\n",
    "for i in tqdm(export_files):\n",
    "    sfile = bz2.BZ2File(path_w2 + export_files[i][1] +'_dropped_w1_5pct', 'w') #'_w1_1pct'\n",
    "    pickle.dump(export_files[i][0],sfile)\n",
    "    sfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce230e8",
   "metadata": {},
   "source": [
    "### A. Get unique values in list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303bdf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_values = []\n",
    "single_values = []\n",
    "for col in X.columns:\n",
    "    unique_values.append(X[col].unique())\n",
    "\n",
    "for i in unique_values:\n",
    "    for j in i:\n",
    "        single_values.append(j)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a938bc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining X and y matrices\n",
    "X = df_prep_dropped.drop(['user_response','clicks', 'index'],axis=1)\n",
    "y = df_prep_dropped['user_response']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1f02376",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.to_csv(path + 'ycomp_pearson_w2_1pct.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0877894d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:14<00:00,  7.06s/it]\n"
     ]
    }
   ],
   "source": [
    "# transform train and test files to pickle form\n",
    "export_files = {1:(X, 'Xcomp'), 2:(y,'ycomp')}\n",
    "\n",
    "for i in tqdm(export_files):\n",
    "    sfile = bz2.BZ2File(path + export_files[i][1] +'_pearson_w2_1pct', 'w') #'_w1_1pct'\n",
    "    pickle.dump(export_files[i][0],sfile)\n",
    "    sfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
